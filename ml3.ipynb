{
 "cells": [
  {
   "cell_type": "raw",
   "id": "72f76df6-f1b5-4fd4-969c-a3341c0278d9",
   "metadata": {},
   "source": [
    "Q1. What is the Filter method in feature selection, and how does it work?\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a9d2d76a-06ab-4029-911f-92095d819d53",
   "metadata": {},
   "source": [
    "Ans:-\n",
    "Filter methods are generally used as a preprocessing step. The selection of features is independent of any machine learning algorithms. Instead, features are selected on the basis of their scores in various statistical tests for their correlation with the outcome variable.\n",
    "In this method, features are filtered based on general characteristics (some metric such as correlation) of the dataset such correlation with the dependent variable. Filter method is performed without any predictive model. It is faster and usually the better approach when the number of features are huge.\n",
    "The filter() method is an iterative method. It calls a provided callbackFn function once for each element in an array, and constructs a new array of all the values for which callbackFn returns a truthy value. Array elements which do not pass the callbackFn test are not included in the new array."
   ]
  },
  {
   "cell_type": "raw",
   "id": "789078b5-df3f-42c7-aa16-973b2686cf95",
   "metadata": {},
   "source": [
    "Q2. How does the Wrapper method differ from the Filter method in feature selection?\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f0f7a5a2-2e1c-4fd5-b25e-3051486353e0",
   "metadata": {},
   "source": [
    "Ans:-\n",
    "The main differences between the filter and wrapper methods for feature selection are: Filter methods measure the relevance of features by their correlation with dependent variable while wrapper methods measure the usefulness of a subset of feature by actually training a model on it."
   ]
  },
  {
   "cell_type": "raw",
   "id": "f8ad9ec3-745f-4689-8af5-a621d8c74f8e",
   "metadata": {},
   "source": [
    "Q3. What are some common techniques used in Embedded feature selection methods?\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7ad95676-5d9e-42ff-9a3b-3ef8ee678e8a",
   "metadata": {},
   "source": [
    "Ans:-\n",
    "\n",
    "Some techniques used are:\n",
    "\n",
    "Regularization – This method adds a penalty to different parameters of the machine learning model to avoid over-fitting of the model. This approach of feature selection uses Lasso (L1 regularization) and Elastic nets (L1 and L2 regularization). The penalty is applied over the coefficients, thus bringing down some coefficients to zero. The features having zero coefficient can be removed from the dataset.\n",
    "\n",
    "Tree-based methods – These methods such as Random Forest, Gradient Boosting provides us feature importance as a way to select features as well. Feature importance tells us which features are more important in making an impact on the target feature."
   ]
  },
  {
   "cell_type": "raw",
   "id": "8742fc9b-d2f9-447d-89fc-3d12b4dee2d0",
   "metadata": {},
   "source": [
    "Q4. What are some drawbacks of using the Filter method for feature selection?\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5e9f15c4-5b8e-4537-bc8a-fb35f7feb6df",
   "metadata": {},
   "source": [
    "Ans:-\n",
    "The common disadvantage of filter methods is that they ignore the interaction with the classifier and each feature is considered independently thus ignoring feature dependencies In addition, it is not clear how to determine the threshold point for rankings to select only the required features and exclude noise."
   ]
  },
  {
   "cell_type": "raw",
   "id": "e0b4641c-559b-4c80-a4c5-7fadb18c1ac2",
   "metadata": {},
   "source": [
    "Q5. In which situations would you prefer using the Filter method over the Wrapper method for feature selection?\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "539e2b21-5515-441a-97c3-6c3904633313",
   "metadata": {},
   "source": [
    "Ans:-\n",
    "For large data you should use the Filter approaches because these approaches are rapid and for small size of data it is better to use Wrapper (KNN, SVM,...) approaches because they are slower than the Filter approaches. or you can combine the two approaches to have better results than the two approaches."
   ]
  },
  {
   "cell_type": "raw",
   "id": "a73a0df3-9dc6-4b0c-9da5-b0ce3d042707",
   "metadata": {},
   "source": [
    "Q6. In a telecom company, you are working on a project to develop a predictive model for customer churn. You are unsure of which features to include in the model because the dataset contains several different ones. Describe how you would choose the most pertinent attributes for the model using the Filter Method.\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e87d9eb2-0564-4858-bccc-4c1a6195e28d",
   "metadata": {},
   "source": [
    "Ans:-\n",
    "Information Gain – It is defined as the amount of information provided by the feature for identifying the target value and measures reduction in the entropy values.\n",
    "\n",
    "Chi-square test — Chi-square method (X2) is generally used to test the relationship between categorical variables. It compares the observed values from different attributes of the dataset to its expected value.\n",
    "\n",
    "Fisher’s Score – Fisher’s Score selects each feature independently according to their scores under Fisher criterion leading to a suboptimal set of features.\n",
    "\n",
    "Correlation Coefficient – Pearson’s Correlation Coefficient is a measure of quantifying the association between the two continuous variables and the direction of the relationship with its values ranging from -1 to 1.\n",
    "\n",
    "Variance Threshold – It is an approach where all features are removed whose variance doesn’t meet the specific threshold. \n",
    "\n",
    "Relief – This method measures the quality of attributes by randomly sampling an instance from the dataset and updating each feature and distinguishing between instances that are near to each other based on the difference between the selected instance and two nearest instances of same and opposite classes."
   ]
  },
  {
   "cell_type": "raw",
   "id": "d306e0f7-6d99-4ce6-abfb-ea792ccc4f0b",
   "metadata": {},
   "source": [
    "Q7. You are working on a project to predict the outcome of a soccer match. You have a large dataset with many features, including player statistics and team rankings. Explain how you would use the Embedded method to select the most relevant features for the model.\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "74cddd65-b961-4bec-a579-20fd14ba8a3b",
   "metadata": {},
   "source": [
    "Ans:-\n",
    "\n",
    "Some techniques used are:\n",
    "Regularization – This method adds a penalty to different parameters of the machine learning model to avoid over-fitting of the model. This approach of feature selection uses Lasso (L1 regularization) and Elastic nets (L1 and L2 regularization). The penalty is applied over the coefficients, thus bringing down some coefficients to zero. The features having zero coefficient can be removed from the dataset.\n",
    "Tree-based methods – These methods such as Random Forest, Gradient Boosting provides us feature importance as a way to select features as well. Feature importance tells us which features are more important in making an impact on the target feature."
   ]
  },
  {
   "cell_type": "raw",
   "id": "c35167cd-8b07-42d3-b58f-9b64aff163d5",
   "metadata": {},
   "source": [
    "Q8. You are working on a project to predict the price of a house based on its features, such as size, location, and age. You have a limited number of features, and you want to ensure that you select the most important ones for the model. Explain how you would use the Wrapper method to select the best set of features for the predictor."
   ]
  },
  {
   "cell_type": "raw",
   "id": "7d9fa2e2-a04f-4d0e-9129-5b0d17646220",
   "metadata": {},
   "source": [
    "Ans:-\n",
    "Forward selection – This method is an iterative approach where we initially start with an empty set of features and keep adding a feature which best improves our model after each iteration.\n",
    "\n",
    "Backward elimination – This method is also an iterative approach where we initially start with all features and after each iteration, we remove the least significant feature\n",
    "\n",
    "Bi-directional elimination – This method uses both forward selection and backward elimination technique simultaneously to reach one unique solution.\n",
    "\n",
    "Exhaustive selection – This technique is considered as the brute force approach for the evaluation of feature subsets. It creates all possible subsets and builds a learning algorithm for each subset and selects the subset whose model’s performance is best."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
